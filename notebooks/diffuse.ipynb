{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import OpenEXR\n",
    "\n",
    "from Imath import PixelType\n",
    "from PIL import Image\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import *\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_inpaint import StableDiffusionInpaintPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "from scipy.sparse import bsr_array, csr_array, save_npz, load_npz\n",
    "\n",
    "%cd ../ext/Text2Light\n",
    "from sritmo.global_sritmo import SRiTMO\n",
    "%cd ../../notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myStableDiffusionImg2ImgPipeline(StableDiffusionImg2ImgPipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: CLIPTextModel,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        unet: UNet2DConditionModel,\n",
    "        scheduler: KarrasDiffusionSchedulers,\n",
    "        safety_checker: StableDiffusionSafetyChecker,\n",
    "        feature_extractor: CLIPImageProcessor,\n",
    "        image_encoder: CLIPVisionModelWithProjection = None,\n",
    "        requires_safety_checker: bool = True,\n",
    "    ):\n",
    "        super(myStableDiffusionImg2ImgPipeline, self).__init__(\n",
    "            vae, text_encoder, tokenizer, unet, scheduler, safety_checker, feature_extractor, image_encoder, requires_safety_checker\n",
    "        )\n",
    "\n",
    "        self.mask_processor = VaeImageProcessor(\n",
    "            vae_scale_factor=self.vae_scale_factor, do_normalize=False, do_binarize=True, do_convert_grayscale=True\n",
    "        )\n",
    "\n",
    "    def _encode_vae_image(self, image: torch.Tensor, generator: torch.Generator):\n",
    "        if isinstance(generator, list):\n",
    "            image_latents = [\n",
    "                retrieve_latents(self.vae.encode(image[i : i + 1]), generator=generator[i])\n",
    "                for i in range(image.shape[0])\n",
    "            ]\n",
    "            image_latents = torch.cat(image_latents, dim=0)\n",
    "        else:\n",
    "            image_latents = retrieve_latents(self.vae.encode(image), generator=generator)\n",
    "\n",
    "        image_latents = self.vae.config.scaling_factor * image_latents\n",
    "\n",
    "        return image_latents\n",
    "\n",
    "    def prepare_mask_latents(\n",
    "        self, mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance\n",
    "    ):\n",
    "        # resize the mask to latents shape as we concatenate the mask to the latents\n",
    "        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n",
    "        # and half precision\n",
    "        mask = torch.nn.functional.interpolate(\n",
    "            mask, size=(height // self.vae_scale_factor, width // self.vae_scale_factor)\n",
    "        )\n",
    "        mask = mask.to(device=device, dtype=dtype)\n",
    "\n",
    "        masked_image = masked_image.to(device=device, dtype=dtype)\n",
    "\n",
    "        if masked_image.shape[1] == 4:\n",
    "            masked_image_latents = masked_image\n",
    "        else:\n",
    "            masked_image_latents = self._encode_vae_image(masked_image, generator=generator)\n",
    "\n",
    "        # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method\n",
    "        if mask.shape[0] < batch_size:\n",
    "            if not batch_size % mask.shape[0] == 0:\n",
    "                raise ValueError(\n",
    "                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n",
    "                    f\" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number\"\n",
    "                    \" of masks that you pass is divisible by the total requested batch size.\"\n",
    "                )\n",
    "            mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)\n",
    "        if masked_image_latents.shape[0] < batch_size:\n",
    "            if not batch_size % masked_image_latents.shape[0] == 0:\n",
    "                raise ValueError(\n",
    "                    \"The passed images and the required batch size don't match. Images are supposed to be duplicated\"\n",
    "                    f\" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed.\"\n",
    "                    \" Make sure the number of images that you pass is divisible by the total requested batch size.\"\n",
    "                )\n",
    "            masked_image_latents = masked_image_latents.repeat(batch_size // masked_image_latents.shape[0], 1, 1, 1)\n",
    "\n",
    "        mask = torch.cat([mask] * 2) if do_classifier_free_guidance else mask\n",
    "        masked_image_latents = (\n",
    "            torch.cat([masked_image_latents] * 2) if do_classifier_free_guidance else masked_image_latents\n",
    "        )\n",
    "\n",
    "        # aligning device to prevent device errors when concating it with the latent model input\n",
    "        masked_image_latents = masked_image_latents.to(device=device, dtype=dtype)\n",
    "        return mask, masked_image_latents\n",
    "    \n",
    "    def prepare_latents(\n",
    "        self,\n",
    "        batch_size,\n",
    "        num_channels_latents,\n",
    "        height,\n",
    "        width,\n",
    "        dtype,\n",
    "        device,\n",
    "        generator,\n",
    "        latents=None,\n",
    "        image=None,\n",
    "        timestep=None,\n",
    "        is_strength_max=True,\n",
    "        return_noise=False,\n",
    "        return_image_latents=False,\n",
    "    ):\n",
    "        shape = (\n",
    "            batch_size,\n",
    "            num_channels_latents,\n",
    "            int(height) // self.vae_scale_factor,\n",
    "            int(width) // self.vae_scale_factor,\n",
    "        )\n",
    "        if isinstance(generator, list) and len(generator) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "            )\n",
    "\n",
    "        if (image is None or timestep is None) and not is_strength_max:\n",
    "            raise ValueError(\n",
    "                \"Since strength < 1. initial latents are to be initialised as a combination of Image + Noise.\"\n",
    "                \"However, either the image or the noise timestep has not been provided.\"\n",
    "            )\n",
    "\n",
    "        if return_image_latents or (latents is None and not is_strength_max):\n",
    "            image = image.to(device=device, dtype=dtype)\n",
    "\n",
    "            if image.shape[1] == 4:\n",
    "                image_latents = image\n",
    "            else:\n",
    "                image_latents = self._encode_vae_image(image=image, generator=generator)\n",
    "            image_latents = image_latents.repeat(batch_size // image_latents.shape[0], 1, 1, 1)\n",
    "\n",
    "        if latents is None:\n",
    "            noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "            # if strength is 1. then initialise the latents to noise, else initial to image + noise\n",
    "            latents = noise if is_strength_max else self.scheduler.add_noise(image_latents, noise, timestep)\n",
    "            # if pure noise then scale the initial latents by the  Scheduler's init sigma\n",
    "            latents = latents * self.scheduler.init_noise_sigma if is_strength_max else latents\n",
    "        else:\n",
    "            noise = latents.to(device)\n",
    "            latents = noise * self.scheduler.init_noise_sigma\n",
    "\n",
    "        outputs = (latents,)\n",
    "\n",
    "        if return_noise:\n",
    "            outputs += (noise,)\n",
    "\n",
    "        if return_image_latents:\n",
    "            outputs += (image_latents,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        image: PipelineImageInput = None,\n",
    "        strength: float = 0.8,\n",
    "        num_inference_steps: Optional[int] = 50,\n",
    "        timesteps: List[int] = None,\n",
    "        guidance_scale: Optional[float] = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: Optional[float] = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        ip_adapter_image: Optional[PipelineImageInput] = None,\n",
    "        ip_adapter_image_embeds: Optional[List[torch.FloatTensor]] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        clip_skip: int = None,\n",
    "        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        # MASK\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        padding_mask_crop: Optional[int] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        mask_image: PipelineImageInput = None,\n",
    "        masked_image_latents: torch.FloatTensor = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        callback = kwargs.pop(\"callback\", None)\n",
    "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "        if callback is not None:\n",
    "            deprecate(\n",
    "                \"callback\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "        if callback_steps is not None:\n",
    "            deprecate(\n",
    "                \"callback_steps\",\n",
    "                \"1.0.0\",\n",
    "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "            )\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(\n",
    "            prompt,\n",
    "            strength,\n",
    "            callback_steps,\n",
    "            negative_prompt,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            ip_adapter_image,\n",
    "            ip_adapter_image_embeds,\n",
    "            callback_on_step_end_tensor_inputs,\n",
    "        )\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._cross_attention_kwargs = cross_attention_kwargs\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        text_encoder_lora_scale = (\n",
    "            self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
    "        )\n",
    "        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n",
    "            prompt,\n",
    "            device,\n",
    "            num_images_per_prompt,\n",
    "            self.do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            lora_scale=text_encoder_lora_scale,\n",
    "            clip_skip=self.clip_skip,\n",
    "        )\n",
    "        # For classifier free guidance, we need to do two forward passes.\n",
    "        # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "        # to avoid doing two forward passes\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "\n",
    "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
    "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
    "                ip_adapter_image,\n",
    "                ip_adapter_image_embeds,\n",
    "                device,\n",
    "                batch_size * num_images_per_prompt,\n",
    "                self.do_classifier_free_guidance,\n",
    "            )\n",
    "\n",
    "        # 4. Preprocess image\n",
    "        if padding_mask_crop is not None:\n",
    "            crops_coords = self.mask_processor.get_crop_region(mask_image, width, height, pad=padding_mask_crop)\n",
    "            resize_mode = \"fill\"\n",
    "        else:\n",
    "            crops_coords = None\n",
    "            resize_mode = \"default\"\n",
    "\n",
    "        init_image = self.image_processor.preprocess(\n",
    "            image, height=height, width=width, crops_coords=crops_coords, resize_mode=resize_mode\n",
    "        )\n",
    "        image = init_image.to(dtype=torch.float32)\n",
    "\n",
    "        # 5. set timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)\n",
    "        timesteps, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n",
    "        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n",
    "\n",
    "        # 6. Prepare latent variables\n",
    "        num_channels_latents = self.vae.config.latent_channels\n",
    "        num_channels_unet = self.unet.config.in_channels\n",
    "        return_image_latents = num_channels_unet == 4\n",
    "\n",
    "        is_strength_max = strength == 1.0\n",
    "\n",
    "        latents_outputs = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "            image=init_image,\n",
    "            timestep=latent_timestep,\n",
    "            is_strength_max=is_strength_max,\n",
    "            return_noise=True,\n",
    "            return_image_latents=return_image_latents,\n",
    "        )\n",
    "\n",
    "        if return_image_latents:\n",
    "            latents, noise, image_latents = latents_outputs\n",
    "        else:\n",
    "            latents, noise = latents_outputs\n",
    "\n",
    "        # 6.1 Prepare mask latent variables\n",
    "        mask_condition = self.mask_processor.preprocess(\n",
    "            mask_image, height=height, width=width, resize_mode=resize_mode, crops_coords=crops_coords\n",
    "        )\n",
    "\n",
    "        if masked_image_latents is None:\n",
    "            masked_image = init_image * (mask_condition < 0.5)\n",
    "        else:\n",
    "            masked_image = masked_image_latents\n",
    "\n",
    "        mask, masked_image_latents = self.prepare_mask_latents(\n",
    "            mask_condition,\n",
    "            masked_image,\n",
    "            batch_size * num_images_per_prompt,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            self.do_classifier_free_guidance,\n",
    "        )\n",
    "\n",
    "        # 6.2 Check that sizes of mask, masked image and latents match\n",
    "        if num_channels_unet == 9:\n",
    "            # default case for runwayml/stable-diffusion-inpainting\n",
    "            num_channels_mask = mask.shape[1]\n",
    "            num_channels_masked_image = masked_image_latents.shape[1]\n",
    "            if num_channels_latents + num_channels_mask + num_channels_masked_image != self.unet.config.in_channels:\n",
    "                raise ValueError(\n",
    "                    f\"Incorrect configuration settings! The config of `pipeline.unet`: {self.unet.config} expects\"\n",
    "                    f\" {self.unet.config.in_channels} but received `num_channels_latents`: {num_channels_latents} +\"\n",
    "                    f\" `num_channels_mask`: {num_channels_mask} + `num_channels_masked_image`: {num_channels_masked_image}\"\n",
    "                    f\" = {num_channels_latents+num_channels_masked_image+num_channels_mask}. Please verify the config of\"\n",
    "                    \" `pipeline.unet` or your `mask_image` or `image` input.\"\n",
    "                )\n",
    "        elif num_channels_unet != 4:\n",
    "            raise ValueError(\n",
    "                f\"The unet {self.unet.__class__} should have either 4 or 9 input channels, not {self.unet.config.in_channels}.\"\n",
    "            )\n",
    "\n",
    "        # 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7.1 Add image embeds for IP-Adapter\n",
    "        added_cond_kwargs = (\n",
    "            {\"image_embeds\": image_embeds}\n",
    "            if ip_adapter_image is not None or ip_adapter_image_embeds is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # 7.2 Optionally get Guidance Scale Embedding\n",
    "        timestep_cond = None\n",
    "        if self.unet.config.time_cond_proj_dim is not None:\n",
    "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "            timestep_cond = self.get_guidance_scale_embedding(\n",
    "                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
    "            ).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        self._num_timesteps = len(timesteps)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                if num_channels_unet == 9:\n",
    "                    latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = self.unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    timestep_cond=timestep_cond,\n",
    "                    cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                    added_cond_kwargs=added_cond_kwargs,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "\n",
    "                if num_channels_unet == 4:\n",
    "                    init_latents_proper = image_latents\n",
    "                    if self.do_classifier_free_guidance:\n",
    "                        init_mask, _ = mask.chunk(2)\n",
    "                    else:\n",
    "                        init_mask = mask\n",
    "\n",
    "                    if i < len(timesteps) - 1:\n",
    "                        noise_timestep = timesteps[i + 1]\n",
    "                        init_latents_proper = self.scheduler.add_noise(\n",
    "                            init_latents_proper, noise, torch.tensor([noise_timestep])\n",
    "                        )\n",
    "\n",
    "                    latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                        callback(step_idx, t, latents)\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False, generator=generator)[\n",
    "                0\n",
    "            ]\n",
    "            image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)\n",
    "        else:\n",
    "            image = latents\n",
    "            has_nsfw_concept = None\n",
    "\n",
    "        if has_nsfw_concept is None:\n",
    "            do_denormalize = [True] * image.shape[0]\n",
    "        else:\n",
    "            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n",
    "\n",
    "        image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n",
    "\n",
    "        # Offload all models\n",
    "        self.maybe_free_model_hooks()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = myStableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors = True,\n",
    "    safety_checker=None\n",
    ")\n",
    "my_pipeline.unet.load_attn_procs(\"../scripts/finetune_lora_pq\")\n",
    "my_pipeline.to('cuda')\n",
    "\n",
    "inpaint_pipeline = my_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_pipeline = myStableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "#     'runwayml/stable-diffusion-v1-5', torch_dtype=torch.float16, variant='fp16', use_safetensors=True, safety_checker=None\n",
    "# ).to('cuda')\n",
    "# my_pipeline.enable_model_cpu_offload()\n",
    "\n",
    "# inpaint_pipeline = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "#     'runwayml/stable-diffusion-v1-5', torch_dtype=torch.float16, variant='fp16', use_safetensors=True, safety_checker=None\n",
    "# ).to('cuda')\n",
    "# inpaint_pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Equirectangular environment map'\n",
    "negative_prompt = 'boxes, artifacts'\n",
    "\n",
    "\n",
    "def load_exr(filename):\n",
    "    exr = OpenEXR.InputFile(filename)\n",
    "    dw = exr.header()['dataWindow']\n",
    "    width = dw.max.x - dw.min.x + 1\n",
    "    height = dw.max.y - dw.min.y + 1\n",
    "\n",
    "    img = np.zeros((height, width, 3), dtype=np.float32)\n",
    "    for i, c in enumerate('RGB'):\n",
    "        buffer = exr.channel(c, PixelType(OpenEXR.FLOAT))\n",
    "        img[:, :, i] = np.frombuffer(buffer, dtype=np.float32).reshape(height, width)\n",
    "\n",
    "    exr.close()\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_exr(img, filename):\n",
    "    height, width, _ = img.shape\n",
    "    header = OpenEXR.Header(width, height)\n",
    "    exr = OpenEXR.OutputFile(filename, header)\n",
    "\n",
    "    r, g, b = np.split(img, 3, axis=-1)\n",
    "    exr.writePixels({'R': r.tobytes(),\n",
    "\t                 'G': g.tobytes(),\n",
    "\t                 'B': b.tobytes()})\n",
    "    exr.close()\n",
    "\n",
    "\n",
    "def rgb_to_srgb(rgb):\n",
    "    srgb = np.where(rgb <= 0.0031308,\n",
    "                  12.92 * rgb,\n",
    "                  (1 + 0.055) * np.power(rgb, 1 / 2.4) - 0.055)\n",
    "    # srgb = np.clip(srgb * 255, 0, 255).astype(np.uint8)\n",
    "    return srgb\n",
    "\n",
    "\n",
    "def srgb_to_rgb(srgb):\n",
    "    rgb = np.where(srgb <= 0.04045,\n",
    "                   srgb / 12.92,\n",
    "                   np.power((srgb + 0.055) / (1 + 0.055), 2.4))\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def rgb_to_hlg(rgb):\n",
    "    hlg = np.where(rgb <= 1.0,\n",
    "                   0.5 * np.sqrt(rgb),\n",
    "                   0.17883277 * np.log(rgb - 0.28466892) + 0.55991073)\n",
    "    return hlg\n",
    "\n",
    "\n",
    "def hlg_to_rgb(hlg):\n",
    "    rgb = np.where(hlg <= 0.5,\n",
    "                   np.square(2.0 * hlg),\n",
    "                   np.exp((hlg - 0.55991073) / 0.17883277) + 0.28466892)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def luminance(rgb):\n",
    "    return rgb @ np.asarray((0.2126, 0.7152, 0.0722), dtype=np.float32)\n",
    "\n",
    "\n",
    "def rgb_to_reinhard(rgb, white_lum=100):\n",
    "    lum = luminance(rgb)\n",
    "    numerator = lum * (1.0 + lum / white_lum ** 2)\n",
    "    new_lum = numerator / (1.0 + lum)\n",
    "\n",
    "    return rgb * np.where(lum != 0, (new_lum / lum), 0)[:, :, None]\n",
    "\n",
    "\n",
    "def reinhard_to_rgb(reinhard, white_lum=100):\n",
    "    new_lum = luminance(reinhard)\n",
    "\n",
    "    # Solve the quadratic equation\n",
    "    a = 1.0 / white_lum ** 2\n",
    "    b = 1.0 - new_lum\n",
    "    c = -new_lum\n",
    "\n",
    "    discriminant = np.sqrt(b ** 2 - 4 * a * c)\n",
    "    lum = (-b + discriminant) / (2 * a)\n",
    "\n",
    "    # Scale reinhard_rgb back to original rgb\n",
    "    return reinhard * np.where(new_lum != 0, (lum / new_lum), 0)[:, :, None]\n",
    "\n",
    "def pq_to_rgb(rgb: np.ndarray, mul: int = 10000) -> np.ndarray:\n",
    "    m1 = 0.1593017578125\n",
    "    m2 = 78.84375\n",
    "    c1 = 0.8359375\n",
    "    c2 = 18.8515625\n",
    "    c3 = 18.6875\n",
    "\n",
    "    ret = None\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        E_p = rgb ** (1/m2)\n",
    "        par = np.maximum((E_p - c1), 0) / (c2 - c3 * E_p)\n",
    "        ret = mul * (par ** (1/m1))\n",
    "        ret[np.isinf(ret)] = 0\n",
    "        ret[np.isnan(ret)] = 0\n",
    "    \n",
    "    return np.clip(ret, 0, 1)\n",
    "\n",
    "def rgb_to_pq(pq: np.ndarray, div: int = 10000) -> np.ndarray:\n",
    "    m1 = 0.1593017578125\n",
    "    m2 = 78.84375\n",
    "    c1 = 0.8359375\n",
    "    c2 = 18.8515625\n",
    "    c3 = 18.6875\n",
    "\n",
    "    ret = None\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        Y = pq / div\n",
    "        ret = ((c1 + c2 * (Y ** m1)) / (1 + c3 * (Y ** m1))) ** m2\n",
    "        ret[np.isinf(ret)] = 0\n",
    "        ret[np.isnan(ret)] = 0\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoselezionare le maschere con il metodo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setto dimensioni dell'immagine env\n",
    "ENV_HEIGHT, ENV_WIDTH = 256, 512\n",
    "\n",
    "# Carico Array di tutte le combinazioni della scena 3d di blender\n",
    "T_bsr = bsr_array(load_npz(\"../T_bsr.npz\"))\n",
    "T = T_bsr.toarray()\n",
    "\n",
    "# Carico l'exr dell'immagine da inpaintare ORIGINALE senza modifiche\n",
    "L = load_exr(\"../resources/meadow_2_90deg.exr\")\n",
    "L = L[:ENV_HEIGHT // 2, :, :].reshape(-1, 3)\n",
    "\n",
    "B = T_bsr @ L * 1e-4\n",
    "\n",
    "# TODO: Come mai ha bisogno di coeff_slices?\n",
    "# B = B.reshape(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "# coeffs = pywt.array_to_coeffs(B, coeff_slices, output_format='wavedec2')\n",
    "# B = pywt.waverec2(coeffs, 'haar', axes=(0, 1))\n",
    "\n",
    "# Carico l'immagine dove seleziono con il colore ROSSO l'ombra e VERDE il resto\n",
    "stroke_img = load_exr(f\"../resources/stroke.exr\").reshape(-1, 3)\n",
    "in_stroke = (1, 0, 0) # RED per l'ombra\n",
    "out_stroke = (0, 1, 0) # GREEN per il resto\n",
    "\n",
    "# Seleziono i pixel che sono rossi e verdi\n",
    "X_in = np.all(stroke_img == in_stroke, axis=-1)\n",
    "X_out = np.all(stroke_img == out_stroke, axis=-1)\n",
    "\n",
    "T_in = np.mean(T[X_in, :], axis=0)\n",
    "T_out = np.mean(T[X_out, :], axis=0)\n",
    "delta = T_out[:, None] * L - T_in[:, None] * L\n",
    "\n",
    "# Carichiamo l'albedo dell'immagine\n",
    "p = load_exr('../resources/albedo.exr').reshape(-1, 3)\n",
    "\n",
    "L_avg = np.mean(L, axis=0)\n",
    "p_avg = np.mean(p, axis=0)\n",
    "\n",
    "L_f = np.any(delta > 0.8 * L_avg * p_avg, axis=-1)\n",
    "L_b = ~L_f\n",
    "\n",
    "env = L.copy()\n",
    "env[L_f, :] = (1, 0, 0)\n",
    "ENV_HEIGHT, ENV_WIDTH = 256, 256\n",
    "\n",
    "f = np.zeros_like(L, dtype=np.float32)\n",
    "b = np.zeros_like(L, dtype=np.float32)\n",
    "mask = np.zeros_like(L, dtype=np.float32)\n",
    "\n",
    "f[L_f, :] = L[L_f, :]\n",
    "b[L_b, :] = L[L_b, :]\n",
    "\n",
    "f_edit = np.roll(f.reshape(ENV_HEIGHT, ENV_WIDTH, 3), -ENV_WIDTH // 2, axis=1).reshape(-1, 3)\n",
    "L_edit = np.where(f_edit > 0, f_edit, b)\n",
    "\n",
    "mask[L_f, :] = 1.0\n",
    "mask[f_edit > 0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mask_img2img = Image.open('../resources/mask_img2img.png')\n",
    "mask_inpaint = Image.open('../resources/mask_inpaint.png')\n",
    "\n",
    "mask_img2img = inpaint_pipeline.mask_processor.blur(mask_img2img, blur_factor=10)\n",
    "mask_inpaint = inpaint_pipeline.mask_processor.blur(mask_inpaint, blur_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envmap = load_exr('../resources/env_edit.exr')\n",
    "mask_img2img = Image.open('../resources/mask_img2img.png')\n",
    "mask_inpaint = Image.open('../resources/mask_inpaint.png')\n",
    "\n",
    "mask_img2img = inpaint_pipeline.mask_processor.blur(mask_img2img, blur_factor=10)\n",
    "mask_inpaint = inpaint_pipeline.mask_processor.blur(mask_inpaint, blur_factor=10)\n",
    "\n",
    "make_image_grid([Image.fromarray(np.clip(rgb_to_srgb(envmap) * 255, 0, 255).astype(np.uint8)),\n",
    "                  mask_img2img, mask_inpaint], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PQ\n",
    "init_image: np.ndarray = rgb_to_pq(envmap)\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.6, output_type='np').images[0]\n",
    "image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.6, output_type='np').images[0]\n",
    "\n",
    "save_exr(pq_to_rgb(image), './pq.exr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear\n",
    "init_image = envmap\n",
    "\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "# image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "#                     height=256, width=512, strength=0.5, output_type='np').images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_exr(image, '../out/diffuse-res/linear.exr')\n",
    "display(Image.fromarray(np.clip(image * 255, 0, 255).astype(np.uint8)).resize((1024, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sRGB without clipping\n",
    "init_image = rgb_to_srgb(envmap)\n",
    "\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "# image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "#                     height=256, width=512, strength=0.5, output_type='np').images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_exr(srgb_to_rgb(image), '../out/diffuse-res/srgb_no_clipping.exr')\n",
    "display(Image.fromarray(np.clip(image * 255, 0, 255).astype(np.uint8)).resize((1024, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sRGB\n",
    "init_image = np.clip(rgb_to_srgb(envmap), 0, 1)\n",
    "\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "# image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "#                     height=256, width=512, strength=0.5, output_type='np').images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_exr(srgb_to_rgb(image), '../out/diffuse-res/srgb.exr')\n",
    "display(Image.fromarray(np.clip(image * 255, 0, 255).astype(np.uint8)).resize((1024, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sRGB with low exposure\n",
    "exposure = -4\n",
    "init_image = np.clip(rgb_to_srgb(envmap * np.exp2(exposure)), 0, 1)\n",
    "\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "# image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "#                     height=256, width=512, strength=0.5, output_type='np').images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_exr(srgb_to_rgb(image) / np.exp2(exposure), '../out/diffuse-res/srgb_low_exposure.exr')\n",
    "display(Image.fromarray(np.clip(image * 255, 0, 255).astype(np.uint8)).resize((1024, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLG\n",
    "init_image = np.clip(rgb_to_hlg(envmap), 0, 1)\n",
    "\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "# image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "#                     height=256, width=512, strength=0.5, output_type='np').images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_exr(hlg_to_rgb(image), '../out/diffuse-res/hlg.exr')\n",
    "image = rgb_to_srgb(hlg_to_rgb(image))\n",
    "display(Image.fromarray(np.clip(image * 255, 0, 255).astype(np.uint8)).resize((1024, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLG with low exposure\n",
    "exposure = -4\n",
    "\n",
    "init_image = np.clip(rgb_to_hlg(envmap * np.exp2(exposure)), 0, 1)\n",
    "\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "# image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "#                     height=256, width=512, strength=0.5, output_type='np').images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_exr(hlg_to_rgb(image) / np.exp2(exposure), '../out/diffuse-res/hlg_low_exposure.exr')\n",
    "image = rgb_to_srgb(hlg_to_rgb(image))\n",
    "display(Image.fromarray(np.clip(image * 255, 0, 255).astype(np.uint8)).resize((1024, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinhard\n",
    "init_image = np.clip(rgb_to_reinhard(envmap), 0, 1)\n",
    "\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "# image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "#                     height=256, width=512, strength=0.5, output_type='np').images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_exr(reinhard_to_rgb(image), '../out/diffuse-res/reinhard.exr')\n",
    "image = rgb_to_srgb(reinhard_to_rgb(image))\n",
    "display(Image.fromarray(np.clip(image * 255, 0, 255).astype(np.uint8)).resize((1024, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sRGB with SRiTMO\n",
    "init_image = np.clip(rgb_to_srgb(envmap), 0, 1)\n",
    "\n",
    "image = my_pipeline(prompt, image=init_image, mask_image=mask_img2img, negative_prompt=negative_prompt,\n",
    "                    height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "# image = inpaint_pipeline(prompt, image=image, mask_image=mask_inpaint, negative_prompt=negative_prompt,\n",
    "#                     height=256, width=512, strength=0.5, output_type='np').images[0]\n",
    "\n",
    "image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).to('cuda') * 2 - 1\n",
    "ldr, hdr = SRiTMO(image, {'sritmo': '../ext/sritmo.pth', 'sr_factor': 1, 'device': 'cuda'})  # output is in BGR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr = hdr.squeeze(0).permute(1, 2, 0)[:, :, [2, 1, 0]].cpu().numpy()\n",
    "save_exr(hdr, '../out/diffuse-res/srgb_sritmo.exr')\n",
    "\n",
    "ldr = ldr.squeeze(0).permute(1, 2, 0)[:, :, [2, 1, 0]].cpu().numpy() / 2 + 0.5\n",
    "display(Image.fromarray(np.clip(ldr * 255, 0, 255).astype(np.uint8)).resize((1024, 512)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
